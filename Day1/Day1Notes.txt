 FLUME
 FLUME IS A DATA INGESTION TOOL, USEFUL FOR CAPTURING LOGS, FILES, SOCIAL MEDIA DATA AND CAN AGGREGATE THE DATA AND PUSH THE DATA INTO STORAGES LIKE HDFS, KAFKA, ELASTIC SEARCH, CLOUD.
IT WORK WITH THE HELP OF AGENT

WE HAVE TO DEFINE
SOURCE: FROM WHERE THE DATA HAS TO BE PICKED
CHANNEL: WHERE DATA IS INTERMEDIATELY STORED AS EVENTS: MEMORY, DISK
SINK: TARGET

1. WE NEED TO INSTALL FLUME: flume-ng
2. WE NEED TO CONFIGURE THE AGENT CONFIGURATION FILE
3. START THE FLUME AGENT

FLUME SPOOL DIR EXAMPLE
# mkdir testdata

# hdfs dfs -mkdir flumetest

# vi /etc/hadoop/flumespool.conf

agent1.sinks = hdfs_sink
agent1.sources = spool_source
agent1.channels = mem_channel
agent1.channels.mem_channel.type = memory
agent1.channels.mem_channel.capacity = 500
# Define a source for agent1
agent1.sources.spool_source.type = spooldir
agent1.sources.spool_source.spoolDir = /root/testdata
agent1.sources.spool_source.fileHeader = false
agent1.sources.spool_source.fileSuffix = .COMPLETED
# Defining sink for agent1
agent1.sinks.hdfs_sink.type = hdfs
agent1.sinks.hdfs_sink.hdfs.path = /user/root/flumetest
agent1.sinks.hdfs_sink.hdfs.writeFormat=Text
agent1.sinks.hdfs_sink.hdfs.fileType = DataStream
agent1.sources.spool_source.channels = mem_channel
agent1.sinks.hdfs_sink.channel = mem_channel

# flume-ng agent --conf-file /etc/hadoop/flumespool.conf --name agent1

Wait for the acknowledgement source started, now don't close this terminal. 

Open another terminal and create 2 files by names data1 and data2 using vi editor. Now copy the files into testdata directory.

# cp data1  testdata
# cp data2  testdata

Now check from hdfs whether file is copied (it may take few minutes so check after few minutes)

# hdfs  dfs  -ls  flumetest 

 STREAMING
 



















